{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pathlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger():\n",
    "    \n",
    "    # Create a log directory \n",
    "    log_dir = pathlib.Path('../').resolve() / 'logs'\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = log_dir / f\"preprocess_{timestamp}.log\"\n",
    "\n",
    "    logger = logging.getLogger('preprocess')\n",
    "    logger.setLevel(logging.INFO)    \n",
    "\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Remove existing handlers if any\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    \n",
    "    # Create formatter and add to handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)   \n",
    "\n",
    "    return logger     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 20 companies with most transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANYS = [\n",
    "    10000, 101200010, 101410010, 101600010, 102100020, 102700020,\n",
    "    102840020, 103000030, 103338333, 103400030, 103600030,\n",
    "    103700030, 103800030, 104300040, 104400040, 104470040,\n",
    "    104900040, 105100050, 105150050, 107800070\n",
    "]\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(company):\n",
    "    repo_root = pathlib.Path('../').resolve() \n",
    "    trx_data_filename = repo_root / 'data' / 'transactions.csv.gz'\n",
    "\n",
    "    processed__trx_dir = repo_root / 'data' / 'processed' / 'transactions'\n",
    "    processed__trx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    one_company_data_filename = processed__trx_dir / f'transactions_company_{company}.csv'\n",
    "    \n",
    "    if os.path.isfile(one_company_data_filename):\n",
    "        logger.info(f\"Loading existing filtered data for company {company} from {one_company_data_filename}\")\n",
    "        df = pd.read_csv(one_company_data_filename)\n",
    "    else:\n",
    "        logger.info(f\"Filtering transactions for company {company} from {trx_data_filename}\")\n",
    "        data_list = []\n",
    "        chunksize = 10**6  # Process 1 million rows at a time\n",
    "\n",
    "    if not os.path.isfile(trx_data_filename):\n",
    "        msg = f\"Transactions file not found at {trx_data_filename}. Run the download_transactions.sh script first.\"\n",
    "        logger.error(msg)\n",
    "        raise FileNotFoundError(msg)\n",
    "    \n",
    "    # Process in chunks to handle large file\n",
    "    for chunk in tqdm.tqdm(pd.read_csv(trx_data_filename, compression='gzip', chunksize=chunksize)):        \n",
    "        \n",
    "        # Filter for the specified company\n",
    "        company_chunk = chunk.query(\"company=={}\".format(company))          \n",
    "        \n",
    "        if not company_chunk.empty:\n",
    "            data_list.append(company_chunk)\n",
    "\n",
    "    # Combine all chunks and save\n",
    "    if data_list:\n",
    "           df = pd.concat(data_list, axis=0)\n",
    "           logger.info(f\"Saving filtered data for company {company} to {one_company_data_filename}\")\n",
    "           df.to_csv(one_company_data_filename, index=None)\n",
    "    else:\n",
    "        msg = f\"No transactions found for company {company} in the dataset.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(df)} transactions for company {company}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "  df = df.query('purchaseamount>0')\n",
    "  df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "  df['start_date'] = df.groupby('id')['date'].transform('min')\n",
    "\n",
    "  # Compute calibration values\n",
    "  calibration_value = (\n",
    "      df.query('date==start_date').groupby('id')\n",
    "      ['purchaseamount'].sum().reset_index())\n",
    "  calibration_value.columns = ['id', 'calibration_value']\n",
    "\n",
    "  # Compute holdout values\n",
    "  one_year_holdout_window_mask = (\n",
    "      (df['date'] > df['start_date']) &\n",
    "      (df['date'] <= df['start_date'] + np.timedelta64(365, 'D')))\n",
    "  holdout_value = (\n",
    "      df[one_year_holdout_window_mask].groupby('id')\n",
    "      ['purchaseamount'].sum().reset_index())\n",
    "  holdout_value.columns = ['id', 'holdout_value']\n",
    "\n",
    "  # Compute calibration attributes\n",
    "  calibration_attributes = (\n",
    "      df.query('date==start_date').sort_values(\n",
    "          'purchaseamount', ascending=False).groupby('id')[[\n",
    "              'chain', 'dept', 'category', 'brand', 'productmeasure'\n",
    "          ]].first().reset_index())\n",
    "\n",
    "  # Merge dataframes\n",
    "  customer_level_data = (\n",
    "      calibration_value.merge(calibration_attributes, how='left',\n",
    "                              on='id').merge(\n",
    "                                  holdout_value, how='left', on='id'))\n",
    "  customer_level_data['holdout_value'] = (\n",
    "      customer_level_data['holdout_value'].fillna(0.))\n",
    "  categorical_features = ([\n",
    "      'chain', 'dept', 'category', 'brand', 'productmeasure'\n",
    "  ])\n",
    "  customer_level_data[categorical_features] = (\n",
    "      customer_level_data[categorical_features].fillna('UNKNOWN'))\n",
    "\n",
    "  # Specify data types\n",
    "  customer_level_data['log_calibration_value'] = (\n",
    "      np.log(customer_level_data['calibration_value']).astype('float32'))\n",
    "  customer_level_data['chain'] = (\n",
    "      customer_level_data['chain'].astype('category'))\n",
    "  customer_level_data['dept'] = (customer_level_data['dept'].astype('category'))\n",
    "  customer_level_data['brand'] = (\n",
    "      customer_level_data['brand'].astype('category'))\n",
    "  customer_level_data['category'] = (\n",
    "      customer_level_data['category'].astype('category'))\n",
    "  customer_level_data['label'] = (\n",
    "      customer_level_data['holdout_value'].astype('float32'))\n",
    "  return customer_level_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(company):    \n",
    "    logger.info(f\"Processing company {company}\")\n",
    "    \n",
    "    # Load transaction data for this company\n",
    "    transaction_level_data = load_data(company)\n",
    "    \n",
    "    # Process to customer level\n",
    "    customer_level_data = preprocess(transaction_level_data)\n",
    "    \n",
    "    # Set paths relative to repository structure\n",
    "    repo_root = pathlib.Path('..').resolve()\n",
    "    processed__customers_dir = repo_root / 'data' / 'processed' / 'customers'\n",
    "    processed__customers_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save customer level data\n",
    "    customer_level_data_file = processed__customers_dir / f'customer_level_data_company_{company}.csv'\n",
    "    customer_level_data.to_csv(customer_level_data_file, index=None)\n",
    "    \n",
    "    logger.info(f\"Customer data saved to: {customer_level_data_file}\")\n",
    "    \n",
    "    return customer_level_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPool() as p:\n",
    "    _ = p.map(process, COMPANYS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
